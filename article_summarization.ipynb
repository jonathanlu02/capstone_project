{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c588f0a3",
   "metadata": {},
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad49145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    '''\n",
    "    Read each text file into a string\n",
    "    '''\n",
    "    f = open(file, 'r', encoding='utf-8', errors='ignore')\n",
    "    #f = open(file, 'r', encoding='ISO-8859-1')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def folder_list(path):\n",
    "    '''\n",
    "    Reads each text file in a folder and concatenates each file into a bigger string\n",
    "    Parameter 'path' is the path of your local folder\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    text = ''\n",
    "    for infile in filelist:\n",
    "        file = os.path.join(path, infile)\n",
    "        text_data = read_data(file)\n",
    "        text += '\\n ' + text_data\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('articles_3_sources.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f7976",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2236fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run once ##\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f796c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run once, install everything ##\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c91a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf44c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sourced from - https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3\n",
    "def _create_frequency_table(text_string) -> dict:\n",
    "    \"\"\"\n",
    "    we create a dictionary for the word frequency table.\n",
    "    For this, we should only use the words that are not part of the stopWords array.\n",
    "    Removing stop words and making frequency table\n",
    "    Stemmer - an algorithm to bring words to its root word.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text_string)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    return freqTable\n",
    "\n",
    "\n",
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix\n",
    "\n",
    "\n",
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table\n",
    "\n",
    "\n",
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents/float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "\n",
    "\n",
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(),idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbc0e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_sentences(tf_idf_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    score a sentence by its word's TF\n",
    "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    sentenceValue = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentenceValue[sent] = total_score_per_sentence/count_words_in_sentence\n",
    "\n",
    "    return sentenceValue\n",
    "\n",
    "\n",
    "def _find_average_score(sentenceValue) -> int:\n",
    "    \"\"\"\n",
    "    Find the average score from the sentence value dictionary\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sumValues/len(sentenceValue))\n",
    "\n",
    "    return average\n",
    "\n",
    "\n",
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]]>=(threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary\n",
    "\n",
    "# s is a scale for the threshold -> higher values means only higher rated TF-IDF \n",
    "# sentences get chosen (more strict).\n",
    "def run_summarization(text, s):\n",
    "    \"\"\"\n",
    "    :param text: Plain summary_text of long article\n",
    "    :return: summarized summary_text\n",
    "    \"\"\"\n",
    "\n",
    "    '''\n",
    "    We already have a sentence tokenizer, so we just need \n",
    "    to run the sent_tokenize() method to create the array of sentences.\n",
    "    '''\n",
    "    # 1 Sentence Tokenize\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_documents = len(sentences)\n",
    "    #print(sentences)\n",
    "\n",
    "    # 2 Create the Frequency matrix of the words in each sentence.\n",
    "    freq_matrix = _create_frequency_matrix(sentences)\n",
    "    #print(freq_matrix)\n",
    "\n",
    "    '''\n",
    "    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
    "    '''\n",
    "    # 3 Calculate TermFrequency and generate a matrix\n",
    "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "    #print(tf_matrix)\n",
    "\n",
    "    # 4 creating table for documents per words\n",
    "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "    #print(count_doc_per_words)\n",
    "\n",
    "    '''\n",
    "    Inverse document frequency (IDF) is how unique or rare a word is.\n",
    "    '''\n",
    "    # 5 Calculate IDF and generate a matrix\n",
    "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "    #print(idf_matrix)\n",
    "\n",
    "    # 6 Calculate TF-IDF and generate a matrix\n",
    "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    #print(tf_idf_matrix)\n",
    "\n",
    "    # 7 Important Algorithm: score the sentences\n",
    "    sentence_scores = _score_sentences(tf_idf_matrix)\n",
    "    #print(sentence_scores)\n",
    "\n",
    "    # 8 Find the threshold\n",
    "    threshold = _find_average_score(sentence_scores)\n",
    "    #print(threshold)\n",
    "\n",
    "    # 9 Important Algorithm: Generate the summary\n",
    "    summary = _generate_summary(sentences, sentence_scores, s*threshold)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac84fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " But who to fight? Where to start? Who were the ancestors? What tools? They are so cute! Mark your calendar and tune in on Tuesday for this special LIVE celebration of the grand reopening night performance of \"The Lion King\" on @disneyonbroardway TikTok at 7:00 p.m. ET/4 p.m. PT. Capt. ... ... ... ... It is not luxurious. There were no takers. Are you a teacher? Congratulations Lily and Tony! Sink Your Fangs Into... Eerie Entertainment! But there are roadblocks. Delicious! Enjoy! What is Evergrande? Why now? #DisneyWorld50! Yum! What do you need me to do? No worries! Oooh! Congratulations Mika! Yum! ... But what do you do? Its awesome! Nope. Watch the replay below! ... Is she being harmed? You cannot concede. Mark your calendar! What will you brew in yours? Enjoy! But can I guarantee it? Check them out! Can you blame her? And desserts? Why? C.J. Cheers!\n"
     ]
    }
   ],
   "source": [
    "result = run_summarization(data, 5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8759bb",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915d8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run once ##\n",
    "#!pip install tensorflow torch  # make sure either tensorflow or torch is installed\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4602a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "187ffd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Muslim Americans who grew up under the shadow of 9/11 have faced hostility and surveillance . Many have faced suspicion, questions about their faith, doubts over their Americanness . They\\'ve also found ways to fight back against bias, to craft nuanced personal narratives about their identities . Some Muslims chose to dispel misconceptions about personal connections through personal connections . \"There is this sense of being Muslim as a kind of important identity marker,\" says one sociologist .'}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HuggingFace summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", max_length=2**31, truncation=True) # summarization uses BART model\n",
    "summarized = summarizer(data, min_length=75, max_length=300)\n",
    "\n",
    "# Print summarized text\n",
    "print(summarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61424e92",
   "metadata": {},
   "source": [
    "# Transformer Translation (Extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5e8ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base (https://huggingface.co/t5-base)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Die muslimischen Amerikaner, die unter dem Schatten des 11. September aufwuchsen, waren mit Feindseligkeit und Überwachung konfrontiert. Viele waren mit Verdacht, Fragen über ihren Glauben und Zweifeln über ihre Amerikanerschaft konfrontiert.'}]\n"
     ]
    }
   ],
   "source": [
    "# some translation testing with transformer\n",
    "translator = pipeline(\"translation_en_to_de\")\n",
    "print(translator(\"\".join(list(summarized[0].values())), max_length=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f0f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
